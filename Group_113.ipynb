{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 2\n",
    "## Text Pre-Processing & Feature Generation\n",
    "\n",
    "#### Student Name: Akshatha Shivashankar Chindalur\n",
    "#### Student ID: 29996503\n",
    "#### Student Name: Pradnya Alchetti\n",
    "#### Student ID: 29595916\n",
    "\n",
    "Date: 02/09/2019\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.7.11 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include the main libraries you used in your assignment here, e.g.,:\n",
    "* pdfminer(for dataframe, included in Anaconda Python 2.7) \n",
    "* nltk (for regular expression, included in Anaconda Python 2.7) \n",
    "* re (for numpy array, included in Anaconda Python 2.7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Generating a sparse matrix for Paper Bodies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries necessary for generation of the sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pradnya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# The required libraries are imported below.\n",
    "\n",
    "import os\n",
    "from io import StringIO\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import MWETokenizer\n",
    "import re\n",
    "import requests\n",
    "import nltk\n",
    "import nltk.data\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from nltk.probability import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generating the corpus of papers as PDF files.\n",
    "\n",
    "The given dataset containing 200 URLs of published papers is read from a PDF file. From these URLs, the corresponding papers are downloaded into the **papers** directory (folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   This function downloads the paper from its respective URL as a PDF file. The downloaded file is\n",
    "#   is stored in the directory (folder) papers.\n",
    "#\n",
    "#   :param url:    the URL (link) of the website from which the published paper needs to be downloaded.\n",
    "#\n",
    "#   :return contents:   a string (contents of the paper) obtained after processing the PDF file.\n",
    " \n",
    "def write_to_pdf(url):\n",
    "    request_pdf = requests.get(url)\n",
    "    name = request_pdf.headers['content-disposition']\n",
    "    f_name = re.search(\"filename=\\\"(.*)\\\"\", name)\n",
    "\n",
    "    with open('papers/'+ f_name.group(1),'wb') as f_pdf:\n",
    "        f_pdf.write(request_pdf.content)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   This function extracts the contents from the given PDF file\n",
    "#\n",
    "#   :param pdf_path:    the path of the PDF file (paper) from which the data needs to be extracted.\n",
    "#\n",
    "#   :return contents:   a string (contents of the paper) obtained after processing the PDF file.\n",
    "\n",
    "def retrieve_from_pdf(pdf_path):\n",
    "    \n",
    "    resource_manager = PDFResourceManager()\n",
    "    file_fail_handle = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    params = LAParams()\n",
    "    converter = TextConverter(resource_manager, file_fail_handle, codec=codec, laparams=params)\n",
    "    fp = open(pdf_path, 'rb')\n",
    "    pdf_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        pdf_interpreter.process_page(page)\n",
    "\n",
    "    contents = file_fail_handle.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    converter.close()\n",
    "    file_fail_handle.close()\n",
    "    \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Obtain 200 URLs from the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = retrieve_from_pdf('./Group113.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Download papers from their respective URLs as PDF files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./papers'):\n",
    "    os.mkdir('./papers')\n",
    "    \n",
    "urls = re.findall('https:.*', dataset)\n",
    "\n",
    "for each_url in urls:\n",
    "    write_to_pdf(each_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Obtain the bodies of the 200 papers in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#   This function retrieves only the body of the paper (ignores the title, author and references).\n",
    "#   It also performs certain pre-processing steps that clean up the text obtained from the PDF extractor.\n",
    "#\n",
    "#   :param pdf_file:    the PDF file (or paper) that needs to parsed\n",
    "#\n",
    "#   :return paper_body:   a string of the paper body obtained after the intial cleansing process.\n",
    "\n",
    "def get_paper_content(pdf_file, regex):\n",
    "    \n",
    "    # the contents from the PDF file is obtained.\n",
    "    pdf_text = retrieve_from_pdf(pdf_file)\n",
    "    \n",
    "    # from the processed PDF files, only the content that matches the regular \n",
    "    # expression is retrieved\n",
    "    paper_content = re.search(regex, pdf_text).group(1)\n",
    "    \n",
    "    # some of the words continue onto the next line. These words contain a '-' between them, \n",
    "    # whcih can be identified with the regular expression '-\\\\n'. Thus, this is replaced with\n",
    "    # a null character such that the new word is a whole.For instance, the word 'man-\\nually' \n",
    "    # become 'manually' and the word 'compar-\\nisons' becomes 'comparisons'.\n",
    "    paper_content = re.sub('-\\\\n','', paper_content)\n",
    "    \n",
    "    # every new page begins with the special character '\\x0c'. This is removed by replacing it\n",
    "    # with a null character.\n",
    "    paper_content = re.sub('\\\\x0c', '', paper_content)\n",
    "    \n",
    "    # the page numbers can be identified with the the regular expression '\\\\n(\\d+)\\\\n\\\\n' which\n",
    "    # is removed by replacing it with a null character.\n",
    "    paper_content = re.sub('\\\\n(\\d+)\\\\n\\\\n','',paper_content)\n",
    "    \n",
    "    # all the single new line characters are replaced with a space.\n",
    "    paper_content = re.sub('\\n',' ', paper_content)\n",
    "    \n",
    "    # lastly, the a single or multiple inline reference such as [1] or [22, 23] can be identified \n",
    "    # with the regular expression '(\\s\\[(\\d+)(,\\s*\\d+)*\\])' which is then replaced by a null \n",
    "    # character.\n",
    "    paper_content = re.sub('(\\s\\[(\\d+)(,\\s*\\d+)*\\])','', paper_content)\n",
    "\n",
    "\n",
    "    return paper_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Sentence Segmentation: tokens are normalised to lower case except the one appearing in the middle of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   This function segments the given string of text into sentences.\n",
    "#\n",
    "#   :param text:    the pre-processed body of the paper from the downloaded PDF files.\n",
    "#\n",
    "#   :return sentences:   an array of strings - each a sentence from the paper body.\n",
    "\n",
    "def get_sentences(text):\n",
    "    \n",
    "    detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = detector.tokenize(text.strip())\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "#   This function converts all tokens to lower case except the ones appearing in the middle\n",
    "#   of a sentence.\n",
    "#\n",
    "#   :param raw_text:    the cleansed body of the paper after being extracted from \n",
    "#                       its respective PDF file.\n",
    "#\n",
    "#   :return normalise_sentence:   an array of strings - each normalised to lower case.\n",
    "\n",
    "def case_normalisation(raw_text):\n",
    "    \n",
    "    sentences = get_sentences(raw_text)\n",
    "    \n",
    "    normalise_sentence = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        normalise_sentence.append(sentence.replace(sentence[0],sentence[0].lower(),1))\n",
    "\n",
    "    return normalise_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Word Tokenization: using the regular expression \"[A-Za-z]\\w+(?:[-'?]\\w+)?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   This function tokenises the text based on the regular expression \"[A-Za-z]\\w+(?:[-'?]\\w+)?\".\n",
    "#\n",
    "#   :param text:    the text which needs to be tokenized.\n",
    "#\n",
    "#   :return tokens:   an array of unigram tokens (strings).\n",
    "\n",
    "def get_tokens(text):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "#   This function genrates a list of unigram tokens that contain only alphabetic characters for \n",
    "#   every document (or paper) in the corpus.\n",
    "#\n",
    "#   :param text:    the text which needs to be tokenized.\n",
    "#\n",
    "#   :return doc_tokens:   a list of alphabetic tokens for the respective document.\n",
    "\n",
    "def tokenize(normalised_text):\n",
    "    \n",
    "    doc_tokens = []\n",
    "    \n",
    "    for sentence in normalised_text:\n",
    "        tokens = get_tokens(sentence)\n",
    "        doc_tokens.extend(tokens)\n",
    "        \n",
    "    doc_tokens = [token for token in doc_tokens if token.isalpha()]\n",
    "    \n",
    "    return doc_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate the sparse matrix, firstly each document in the corpus has to be tokenised individually. This is done with the help of 4 threads running parallelly. Further speeding up the tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "AttributeError: Can't get attribute 'tokenize_content_from_docs' on <module '__main__'>\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'tokenize_content_from_docs' on <module '__main__'>\n",
      "AttributeError: Can't get attribute 'tokenize_content_from_docs' on <module '__main__'>\n",
      "AttributeError: Can't get attribute 'tokenize_content_from_docs' on <module '__main__'>\n"
     ]
    }
   ],
   "source": [
    "#   This function first segments each document into sentences and then generates a \n",
    "#   corresponding list of tokens\n",
    "#\n",
    "#   :param file_path:    the path of the PDF file that needs to be processed.\n",
    "#\n",
    "#   :return filename:    the name of the file just processed (used as a key)\n",
    "#           tokens_list: list of tokens obtained by processing this file.   \n",
    "\n",
    "def tokenize_paper_body_from_docs(file_path):\n",
    "    \n",
    "    # obtain the name of the document being parsed\n",
    "    name_of_file = os.path.basename(file_path)\n",
    "    \n",
    "    # get only the body of the paper for feature extraction\n",
    "    # This is done with the help of a regular expression that \n",
    "    # identifies the group between Paper Body and References.\n",
    "    paper_body = get_paper_content(file_path, 'Paper Body([\\s\\S]*)\\d\\sReferences')\n",
    "    \n",
    "    # tokens normalised to lower case excluding the ones in the middle of a \n",
    "    # sentence.\n",
    "    normalised_text = case_normalisation(paper_body)\n",
    "    \n",
    "    # list of tokens obtained after processing the document.\n",
    "    list_tokens = tokenize(normalised_text)\n",
    "    \n",
    "    \n",
    "    return name_of_file, list_tokens\n",
    "    \n",
    "import multiprocessing as mp\n",
    "import glob\n",
    "\n",
    "# building a pool of 4 processes\n",
    "pool = mp.Pool(processes = 4) \n",
    "\n",
    "# obtaining the list of file names from the 'papers' directory.\n",
    "filenames = glob.glob('./papers/*.pdf')\n",
    "\n",
    "# dictionary of tokenised documents with key as the file name and value as the list\n",
    "# of tokens.\n",
    "tokenized_data = dict(pool.map(tokenize_paper_body_from_docs, filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "757220"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# genrating a corpus of tokens from all the 200 documents in the 'papers' directory\n",
    "\n",
    "combine_tokens = []\n",
    "\n",
    "for each_doc in tokenized_data.values():\n",
    "    combine_tokens.extend(each_doc)\n",
    "    \n",
    "len(combine_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_data.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Bigrams: extracting 200 meaningful bigrams from the above generated token corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(combine_tokens)\n",
    "bigrams = finder.nbest(bigram_measures.pmi, 300)\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate context independent stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions reads the given set of stop words\n",
    "def get_stopwords():\n",
    "    with open('stopwords_en.txt', 'r') as stop_word_file:\n",
    "        return set(stop_word_file.read().splitlines())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = get_stopwords()\n",
    "\n",
    "bigrams = [(x,y) for x,y in bigrams if (x not in stop and y not in stop)] \n",
    "\n",
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a corpus of words from fetching tokens from all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_of_pdfs = []\n",
    "for each_doc in tokenized_data:\n",
    "    corpus_of_pdfs.extend(tokenized_data[each_doc])\n",
    "len(corpus_of_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will remove the tokens with the length less than 3 \n",
    "\n",
    "def remove_tokens_less_than_len_3(token_list):\n",
    "    for token in token_list:\n",
    "        if len(token) < 3:\n",
    "            token_list.remove(token)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the tokens of each document with the stopwords set given and remove them if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_context_independent_stopwords(tokens_list):\n",
    "    stopwords = get_stopwords()\n",
    "\n",
    "    filtered_context_independent = [] \n",
    "\n",
    "    for w in tokens_list: \n",
    "        if w not in stopwords: \n",
    "            filtered_context_independent.append(w)\n",
    "\n",
    "    return filtered_context_independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize each document tokens to include bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer = MWETokenizer(bigrams)\n",
    "\n",
    "#for tokens in filtered_context_independent:\n",
    "tokenized_data = mwe_tokenizer.tokenize(filtered_context_independent)\n",
    "#for doc,tokens in tokenized_data.items():\n",
    "#    tokenized_data[doc] = mwe_tokenizer.tokenize(tokens)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_tokens = remove_tokens_less_than_len_3(tokenized_data)\n",
    "rare_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming Using Porter Stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stem_tokens = []\n",
    "\n",
    "for w in rare_tokens:\n",
    "    stem_tokens.append(ps.stem(w))\n",
    "    \n",
    "stem_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate context dependent stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context dependent words\n",
    "words_2 = list(chain.from_iterable([set(value) for value in tokenized_data.values()]))\n",
    "fd_2 = FreqDist(words_2)\n",
    "l = fd_2.most_common(800)\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   This function first segments each document into sentences and then generates a \n",
    "#   corresponding list of tokens\n",
    "#\n",
    "#   :param file_path:    the path of the PDF file that needs to be processed.\n",
    "#\n",
    "#   :return filename:    the name of the file just processed (used as a key)\n",
    "#           tokens_list: list of tokens obtained by processing this file.   \n",
    "\n",
    "def tokenize_content_from_docs(regex, content_type, file_path):\n",
    "    \n",
    "    # obtain the name of the document being parsed\n",
    "    name_of_file = os.path.basename(file_path)\n",
    "    \n",
    "    # get only the content of the paper for feature extraction\n",
    "    content = get_paper_content(file_path, regex)\n",
    "    \n",
    "    # Check the content type\n",
    "    if(content_type == \"abstract\"):\n",
    "        \n",
    "        # tokens normalised to lower case excluding the ones in the middle of a \n",
    "        # sentence.\n",
    "        normalised_text = case_normalisation(content)\n",
    "        \n",
    "        # list of tokens obtained after processing the document.\n",
    "        list_tokens = tokenize(normalised_text)\n",
    "       \n",
    "    elif(content_type == \"title\"):\n",
    "        \n",
    "        # tokens are all normalised to lowercase. \n",
    "        normalised_text = content.lower()\n",
    "        \n",
    "        # list of tokens obtained after processing the document.\n",
    "        list_tokens = get_tokens(normalised_text)\n",
    "    \n",
    "    \n",
    "    return name_of_file, list_tokens\n",
    " \n",
    "# you can remove this once the entire code is tested\n",
    "# import multiprocessing as mp\n",
    "# import glob\n",
    "# from functools import partial\n",
    "# # building a pool of 4 processes\n",
    "# pool = mp.Pool(processes = 4) \n",
    "\n",
    "# # obtaining the list of file names from the 'papers' directory.\n",
    "# filenames = glob.glob('./papers/*.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the abstract of the paper for feature extraction\n",
    "# This is done with the help of a regular expression that \n",
    "# identifies the group between Abstract and Paper Body.\n",
    "\n",
    "# dictionary of tokenised documents with key as the file name and value as the list of tokens.\n",
    "\n",
    "tokenized_data_abstract = dict(pool.map(partial(tokenize_content_from_docs, 'Abstract([\\s\\S]*)\\d\\sPaper Body','abstract'), filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the title of the paper for feature extraction\n",
    "# This is done with the help of a regular expression that \n",
    "# identifies the group that starts between A-Z or a-z or '(' and Authored by.\n",
    "\n",
    "# dictionary of tokenised documents with key as the file name and value as the list\n",
    "# of tokens.\n",
    "\n",
    "\n",
    "tokenized_data_title = dict(pool.map(partial(tokenize_content_from_docs, '(^[A-Za-z(][\\s\\S]*)\\\\n\\\\nAuthored by','title'), filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   This function first retrieves the author names from the paper and then generates a \n",
    "#   corresponding list of tokens\n",
    "#\n",
    "#   :param file_path:    the path of the PDF file that needs to be processed.\n",
    "#\n",
    "#   :param regex: the regular expression to be used for parsing the file.\n",
    "#\n",
    "#   :return filename:    the name of the file just processed (used as a key)\n",
    "#           tokens_list: list of tokens obtained by processing this file. \n",
    "def get_authors_list(regex, file_path):\n",
    "    \n",
    "    # obtain the name of the document being parsed\n",
    "    name_of_file = os.path.basename(file_path)\n",
    "    \n",
    "    # the contents from the PDF file is obtained.\n",
    "    pdf_text = retrieve_from_pdf(file_path)\n",
    "    \n",
    "    # from the processed PDF files, extract the content on the basis of the regular expression.\n",
    "    paper_author = re.search(regex, pdf_text).group(1)\n",
    "    \n",
    "    authors = paper_author.split('\\n')\n",
    "    \n",
    "    return name_of_file, authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the authors of the paper for feature extraction\n",
    "# This is done with the help of a regular expression that \n",
    "# identifies the group that starts between Authored by and Abstract.\n",
    "\n",
    "# dictionary of tokenised documents with key as the file name and value as the list\n",
    "# of tokens.\n",
    "\n",
    "\n",
    "authors_data = dict(pool.map(partial(get_authors_list, 'Authored by:([\\s\\S]*)\\\\n\\\\nAbstract'), filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty elements from the authors list\n",
    "for each in authors_data:\n",
    "    author_list = []\n",
    "    for i in range(len(authors_data[each])):\n",
    "        if authors_data[each][i] != '':\n",
    "            author_list.append(authors_data[each][i])\n",
    "    authors_data[each] = author_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(tokenized_data):\n",
    "    for each in tokenized_data:\n",
    "        tokenized_data[each] = filter_context_independent_stopwords(tokenized_data[each])\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After removing the context independent stop words\n",
    "# we retrieve all the words in all the documents and calculate the frequency of each word\n",
    "\n",
    "def get_most_common_words(tokenized_data, content_type):\n",
    "    \n",
    "    # Check content type and filter stopwords for abstract and title\n",
    "    if content_type == 'abstract' or content_type == 'title':\n",
    "        tokenized_data = filter_stopwords(tokenized_data)\n",
    "    \n",
    "    # create a list of words from all the documents\n",
    "    words = list(chain.from_iterable(tokenized_data.values()))\n",
    "    \n",
    "    # retrieve the 10 most common words\n",
    "    freq_dist = FreqDist(words)\n",
    "    most_common = freq_dist.most_common(10)\n",
    "    \n",
    "    # Create a list of top 10 common words\n",
    "    top_ten = []\n",
    "    for word in most_common:\n",
    "        top_ten.append(word[0])\n",
    "    return top_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 10 most occurring terms in abstract\n",
    "top_abstract = get_most_common_words(tokenized_data_abstract,'abstract')\n",
    "\n",
    "# get top 10 most occurring terms in title\n",
    "top_title = get_most_common_words(tokenized_data_title,'title')\n",
    "\n",
    "# get top 10 authors\n",
    "top_author = get_most_common_words(authors_data,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a dataframe for the statistics\n",
    "stats_data = {'top10_terms_in_abstracts':top_abstract,'top10_terms_in_titles':top_title,'top10_authors':top_author}\n",
    "\n",
    "data_frame = pd.DataFrame(stats_data)\n",
    "\n",
    "# write to csv\n",
    "data_frame.to_csv(\"Group113_stats.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary\n",
    "Give a short summary of your work done above, such as your findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
