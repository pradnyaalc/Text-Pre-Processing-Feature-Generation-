{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 2\n",
    "## Text Pre-Processing & Feature Generation\n",
    "\n",
    "#### Student Name: Akshatha Shivashankar Chindalur\n",
    "#### Student ID: 29996503\n",
    "#### Student Name: Pradnya Alchetti\n",
    "#### Student ID: 29595916\n",
    "\n",
    "Date: 15/09/2019\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.7.3 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "\n",
    "* pdfminer (for extracting information in PDFs, included in Anaconda Python 3.7) \n",
    "* nltk (for natural language processing, included in Anaconda Python 3.7) \n",
    "* re (for regular expressions, included in Anaconda Python 3.7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Generating a sparse matrix for Paper Bodies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries necessary for generation of the sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/akshathacs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# The required libraries are imported below.\n",
    "\n",
    "import os\n",
    "from io import StringIO\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import multiprocessing as mp\n",
    "import glob\n",
    "import re\n",
    "import requests\n",
    "import collections\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import *\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generating the corpus of papers as PDF files.\n",
    "\n",
    "The given dataset containing 200 URLs of published papers is read from a PDF file. From these URLs, the corresponding papers are downloaded into the **papers** directory (folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   This function downloads the paper from its respective URL as a PDF file. The downloaded file is\n",
    "#   is stored in the directory (folder) papers.\n",
    "#\n",
    "#   :param url:    the URL (link) of the website from which the published paper needs to be downloaded.\n",
    "#\n",
    "#   :return contents:   a string (contents of the paper) obtained after processing the PDF file.\n",
    " \n",
    "def write_to_pdf(url):\n",
    "    request_pdf = requests.get(url)\n",
    "    name = request_pdf.headers['content-disposition']\n",
    "    f_name = re.search(\"filename=\\\"(.*)\\\"\", name)\n",
    "\n",
    "    with open('papers/'+ f_name.group(1),'wb') as f_pdf:\n",
    "        f_pdf.write(request_pdf.content)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   This function extracts the contents from the given PDF file\n",
    "#\n",
    "#   :param pdf_path:    the path of the PDF file (paper) from which the data needs to be extracted.\n",
    "#\n",
    "#   :return contents:   a string (contents of the paper) obtained after processing the PDF file.\n",
    "\n",
    "def retrieve_from_pdf(pdf_path):\n",
    "    \n",
    "    resource_manager = PDFResourceManager()\n",
    "    file_fail_handle = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    params = LAParams()\n",
    "    converter = TextConverter(resource_manager, file_fail_handle, codec=codec, laparams=params)\n",
    "    fp = open(pdf_path, 'rb')\n",
    "    pdf_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        pdf_interpreter.process_page(page)\n",
    "\n",
    "    contents = file_fail_handle.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    converter.close()\n",
    "    file_fail_handle.close()\n",
    "    \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Obtain 200 URLs from the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200 URLs from the given PDF file are obtained\n",
    "dataset = retrieve_from_pdf('./Group113.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Download papers from their respective URLs as PDF files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a directory called 'papers' to store the downloaded PDFs is created (if not present already)\n",
    "\n",
    "if not os.path.exists('./papers'):\n",
    "    os.mkdir('./papers')\n",
    "    \n",
    "urls = re.findall('https:.*', dataset)\n",
    "\n",
    "for each_url in urls:\n",
    "    write_to_pdf(each_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Obtain the content of the 200 papers in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_text(file_path):\n",
    "    \n",
    "    # obtain the name of the document being parsed\n",
    "    name_of_file = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # the contents from the PDF file is obtained.\n",
    "    pdf_text = retrieve_from_pdf(file_path)\n",
    "    \n",
    "    return name_of_file, pdf_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first retrieve the content in the form of text for each pdf file in the corpus. This is done with the help of 3 threads running parallelly. Further speeding up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a pool of 3 processes\n",
    "pool = mp.Pool(processes = 3) \n",
    "\n",
    "# obtaining the list of file names from the 'papers' directory.\n",
    "filenames = glob.glob('./papers/*.pdf')\n",
    "\n",
    "# dictionary of raw contents obtained from parsing the PDF files of papers.\n",
    "raw_text = dict(pool.map(get_raw_text, filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#   This function retrieves only the content of the paper based on regular expression.\n",
    "#   It also performs certain pre-processing steps that clean up the text obtained from the PDF extractor.\n",
    "#\n",
    "#   :param pdf_text:    the PDF text (or paper) that needs to parsed\n",
    "#\n",
    "#   :return paper_content:   a string of the paper content obtained after applying the regular expression and \n",
    "#   after the intial cleaning process.\n",
    "\n",
    "def get_paper_content(pdf_text, regex):\n",
    "    \n",
    "    # the contents from the PDF file is obtained.\n",
    "    #pdf_text = retrieve_from_pdf(pdf_file)\n",
    "    \n",
    "    # from the processed PDF files, only the content that matches the regular \n",
    "    # expression is retrieved\n",
    "    paper_content = re.search(regex, pdf_text).group(1)\n",
    "    \n",
    "    # some of the words continue onto the next line. These words contain a '-' between them, \n",
    "    # whcih can be identified with the regular expression '-\\\\n'. Thus, this is replaced with\n",
    "    # a null character such that the new word is a whole.For instance, the word 'man-\\nually' \n",
    "    # become 'manually' and the word 'compar-\\nisons' becomes 'comparisons'.\n",
    "    paper_content = re.sub('-\\\\n','', paper_content)\n",
    "    \n",
    "    # every new page begins with the special character '\\x0c'. This is removed by replacing it\n",
    "    # with a null character.\n",
    "    paper_content = re.sub('\\\\x0c', '', paper_content)\n",
    "    \n",
    "    # the page numbers can be identified with the the regular expression '\\\\n(\\d+)\\\\n\\\\n' which\n",
    "    # is removed by replacing it with a null character.\n",
    "    paper_content = re.sub('\\\\n(\\d+)\\\\n\\\\n','',paper_content)\n",
    "    \n",
    "    # all the single new line characters are replaced with a space.\n",
    "    paper_content = re.sub('\\n',' ', paper_content)\n",
    "    \n",
    "    # lastly, the a single or multiple inline reference such as [1] or [22, 23] can be identified \n",
    "    # with the regular expression '(\\s\\[(\\d+)(,\\s*\\d+)*\\])' which is then replaced by a null \n",
    "    # character.\n",
    "    paper_content = re.sub('(\\s\\[(\\d+)(,\\s*\\d+)*\\])','', paper_content)\n",
    "\n",
    "\n",
    "    return paper_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Sentence Segmentation: tokens are normalised to lower case except the one appearing in the middle of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   This function segments the given string of text into sentences.\n",
    "#\n",
    "#   :param text:    the pre-processed body of the paper from the downloaded PDF files.\n",
    "#\n",
    "#   :return sentences:   an array of strings - each a sentence from the paper body.\n",
    "\n",
    "def get_sentences(text):\n",
    "    \n",
    "    detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = detector.tokenize(text.strip())\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "#   This function converts all tokens to lower case except the ones appearing in the middle\n",
    "#   of a sentence.\n",
    "#\n",
    "#   :param raw_text:    the cleansed body of the paper after being extracted from \n",
    "#                       its respective PDF file.\n",
    "#\n",
    "#   :return normalise_sentence:   an array of strings - each normalised to lower case.\n",
    "\n",
    "def case_normalisation(raw_text):\n",
    "    \n",
    "    sentences = get_sentences(raw_text)\n",
    "    \n",
    "    normalise_sentence = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        normalise_sentence.append(sentence.replace(sentence[0],sentence[0].lower(),1))\n",
    "\n",
    "    return normalise_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Word Tokenization: using the regular expression \"[A-Za-z]\\w+(?:[-'?]\\w+)?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   This function tokenises the text based on the regular expression \"[A-Za-z]\\w+(?:[-'?]\\w+)?\".\n",
    "#\n",
    "#   :param text:    the text which needs to be tokenized.\n",
    "#\n",
    "#   :return tokens:   an array of unigram tokens (strings).\n",
    "\n",
    "def get_tokens(text):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "#   This function genrates a list of unigram tokens that contain only alphabetic characters for \n",
    "#   every document (or paper) in the corpus.\n",
    "#\n",
    "#   :param text:    the text which needs to be tokenized.\n",
    "#\n",
    "#   :return doc_tokens:   a list of alphabetic tokens for the respective document.\n",
    "\n",
    "def tokenize(normalised_text):\n",
    "    \n",
    "    doc_tokens = []\n",
    "    \n",
    "    for sentence in normalised_text:\n",
    "        tokens = get_tokens(sentence)\n",
    "        doc_tokens.extend(tokens)\n",
    "        \n",
    "    doc_tokens = [token for token in doc_tokens if token.isalpha()]\n",
    "    \n",
    "    return doc_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate the sparse matrix, firstly each document in the corpus has to be tokenised individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = {}\n",
    "\n",
    "for paper in raw_text:\n",
    "    paper_body = get_paper_content(raw_text[paper], 'Paper Body([\\s\\S]*)\\d+\\sReferences')\n",
    "    normalised_text = case_normalisation(paper_body)\n",
    "    tokenized_data[paper] = tokenize(normalised_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genrating a corpus of tokens from all the 200 documents in the 'papers' directory\n",
    "\n",
    "combine_tokens = []\n",
    "\n",
    "for each_doc in tokenized_data.values():\n",
    "    combine_tokens.extend(each_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Bigrams: extracting 200 meaningful bigrams from the above generated token corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain all the possible bigrams from the token corpus\n",
    "all_bigrams = ngrams(combine_tokens, 2)\n",
    "\n",
    "# calculating the frequency of each bigram within the corpus\n",
    "bigram_freq = collections.Counter(all_bigrams)\n",
    "\n",
    "# obtaining the 3000 most frequent bigrams. From these 3000 bigrams only\n",
    "# the top 200 bigrams (the ones that do not contain the context independent \n",
    "# stopwords and tokens of length less than 3) are retained.\n",
    "bigram_1000 = bigram_freq.most_common(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the given file, 'stopwords_en.txt' which contains the context independent \n",
    "# stop words\n",
    "stopwords_file = open('stopwords_en.txt', 'r')\n",
    "\n",
    "# storing the context independent stop words in a list\n",
    "context_independent = stopwords_file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   This function removes the context independent stop words from the list of tokens\n",
    "#\n",
    "#   :param tokenized_data:    dictionary of tokens with context independent stop words\n",
    "#\n",
    "#   :return tokenized_data:    dictionary of tokens without context independent stop words\n",
    "\n",
    "def filter_stopwords(tokenized_data):\n",
    "    \n",
    "    for each in tokenized_data:\n",
    "        tokenized_data[each] = [w for w in tokenized_data[each] if w.lower() not in context_independent]\n",
    "    \n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the 200 most meaningful bigrams\n",
    "\n",
    "# if any one of the words in the bigram is a context independent stop word, it \n",
    "# is removed from the final list\n",
    "bigram_200 = [(bigram[0],bigram[1]) for bigram, freq in bigram_1000 \n",
    "              if (bigram[0].lower() not in context_independent and bigram[1].lower() not in context_independent)] \n",
    "\n",
    "# if either of the words in a bigram is of length less than 3, then that bigram\n",
    "# is removed from the list\n",
    "bigram_200 = [(bigram[0],bigram[1]) for bigram in bigram_200 if (len(bigram[0]) >= 3) \n",
    "              and (len(bigram[1]) >= 3)]\n",
    "\n",
    "# finally only the most frequent 200 bigrams from the final list are retained \n",
    "# for further processing.\n",
    "bigram_200 = bigram_200[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Context Independent and Dependent Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the meaningful 200 bigrams are included in the corpus of tokens for each document. The bigrams are \n",
    "# joined with a '__' (double underscore) between them.\n",
    "\n",
    "mwe_tokenizer = MWETokenizer(bigram_200, separator='__')\n",
    "\n",
    "uni_bigram = {}\n",
    "\n",
    "for doc,tokens in tokenized_data.items():\n",
    "    uni_bigram[doc] = mwe_tokenizer.tokenize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from each of the tokenised documents the context independent stop words are removed.\n",
    "\n",
    "tokenized_data_no_stop = filter_stopwords(uni_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the document frequence: the number of documents a word appears in is computed\n",
    "\n",
    "# a list of unique tokens from each document. This will help in identifying the number of\n",
    "# documents a word appears in.\n",
    "unique_tokens = list(chain.from_iterable([set(token) for token in tokenized_data_no_stop.values()]))\n",
    "\n",
    "# the frequency of each word is determined\n",
    "freq_utokens = FreqDist(unique_tokens)\n",
    "\n",
    "# since the threshold is 95% of 200 (documents) = 190, the words that appear in more \n",
    "#than 190 documents are deleted.\n",
    "most_freq_tokens = [key for key, value in freq_utokens.items() if value >= 190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both the context depedent and indepedent stop words are removed from the intial corpus\n",
    "# of tokens.\n",
    "no_stop_tokens = {}\n",
    "\n",
    "for key in tokenized_data_no_stop:\n",
    "    no_stop_tokens[key] = [token for token in tokenized_data_no_stop[key] if token.lower() not in most_freq_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Tokens with length less than 3 are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tokens which are of length less than 3 are removed from the token corpus.\n",
    "\n",
    "length_three = {}\n",
    "\n",
    "for key in no_stop_tokens:\n",
    "    length_three[key] = [token for token in no_stop_tokens[key] if (len(token) >= 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Rare Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the threshold for rare tokens to be removed is 3% of 200 documents = 6,\n",
    "# the tokens that appear in 6 or fewer number of documents are removed\n",
    "\n",
    "rare_tokens = [key for key, value in freq_utokens.items() if value <= 6]\n",
    "\n",
    "del_rare_tokens = {}\n",
    "for key in length_three:\n",
    "    del_rare_tokens[key] = [token for token in length_three[key] if token not in rare_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Stemming Unigram Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   This function seperates unigrams and bigrams from the combined corpus\n",
    "#   of each paper (document).\n",
    "#\n",
    "#   :param token_list:    combined list of unigrams and bigrams\n",
    "#\n",
    "#   :return unigram:   a list of unigram tokens.\n",
    "#           bigram:    a list of bigram tokens.\n",
    "\n",
    "def sep_uni(token_list):\n",
    "    \n",
    "    unigram = []\n",
    "    bigram = []\n",
    "    for token in token_list:\n",
    "        if '__' in token:\n",
    "            bigram.append(token)\n",
    "        else:\n",
    "            unigram.append(token)\n",
    "            \n",
    "    return unigram, bigram\n",
    "\n",
    "#   This function performs stemming on the list of unigram tokens. Tokens like Barcelona, USA, HMM\n",
    "#   were being translated to barcelona, usa an hmm respectively. Although, the case of such tokens\n",
    "#   has been retained (capitalised/title) by checking if the token is a Title or Capitalised.\n",
    "#\n",
    "#   :param unigram_list:    list of unigram tokens to be stemmed\n",
    "#\n",
    "#   :return stem_uni:    list of stemmed unigram tokens\n",
    "\n",
    "def stemming(unigram_list):\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    stem_uni = []\n",
    "    \n",
    "    for word in unigram_list:\n",
    "        \n",
    "        # check if first letter is upper case\n",
    "        if word.istitle():\n",
    "            stem_uni.append(ps.stem(word).capitalize())\n",
    "        # check if all/few letters are in upper case\n",
    "        elif word.isupper():\n",
    "            stem_uni.append(ps.stem(word).upper())\n",
    "        else:\n",
    "            stem_uni.append(ps.stem(word))\n",
    "    \n",
    "    return stem_uni\n",
    "\n",
    "stem_tokens = {}\n",
    "\n",
    "# stemming is done for tokens from each document\n",
    "for key in del_rare_tokens:\n",
    "    \n",
    "    # seperate unigrams and bigrams\n",
    "    uni, bi = sep_uni(del_rare_tokens[key])\n",
    "    \n",
    "    # obtain stemmed unigrams\n",
    "    stemmed_uni = stemming(uni)\n",
    "    \n",
    "    # merge the stemmed unigrams and bigrams\n",
    "    stem_tokens[key] = stemmed_uni + bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output 1: Vocabulary Index File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a count vector for converting tokens into a sparse matrix representation\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", lowercase = False) \n",
    "\n",
    "# generating the sparse matrix for the pre-processed text\n",
    "data_features = vectorizer.fit_transform([' '.join(value) for value in stem_tokens.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining the features\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "# writing the features and its list index to the file 'Group113_vocab.txt'\n",
    "with open('Group113_vocab.txt', \"w+\") as vocab:\n",
    "    for i in range(len(features)):\n",
    "        vocab.write(features[i] + ':')\n",
    "        vocab.write(str(i)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output 2: Sparse Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transferring the contents of the sparse matrix to the text file 'Group113_count_vectors.txt'\n",
    "with open('Group113_count_vectors.txt', 'w+') as count_vector:\n",
    "    \n",
    "    # getting the list of paper names\n",
    "    file_names = list(stem_tokens.keys())\n",
    "    \n",
    "    for i in range(data_features.shape[0]):\n",
    "        \n",
    "        # from each row in the sparse matrix only the non zero elements are obtained\n",
    "        row, col = data_features[i].nonzero()\n",
    "        \n",
    "        # the contents of the sparse matrix are written to the file\n",
    "        count_vector.write(file_names[i] + ',')\n",
    "        for j in range(len(col) - 1):\n",
    "            count_vector.write(str(col[j]) + ':')\n",
    "            count_vector.write(str(data_features[i,col[j]])+',')\n",
    "        count_vector.write(str(col[-1]) + ':')\n",
    "        count_vector.write(str(data_features[i,col[-1]])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The abstract of each paper is required for feature generation. This is extracted (with the help\n",
    "# of a regular expression)from the raw text obtained from parsing the PDF files.\n",
    "\n",
    "# dictionary of tokenised documents with key as the file name and value as the list of tokens.\n",
    "tokenized_data_abstract = {}\n",
    "\n",
    "for paper in raw_text:\n",
    "    \n",
    "    # get the abstract of the paper for feature extraction\n",
    "    # regular expression identifies the text between the key words Abstract and Paper Body\n",
    "    paper_abstract = get_paper_content(raw_text[paper], 'Abstract([\\s\\S]*)\\d+\\sPaper Body')\n",
    "    \n",
    "    # tokens normalised to lower case excluding the ones in the middle of a \n",
    "    # sentence.\n",
    "    normalised_text = case_normalisation(paper_abstract)\n",
    "    \n",
    "    # list of tokens obtained after processing the document are assigned to the respective document\n",
    "    tokenized_data_abstract[paper] = tokenize(normalised_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Title of each paper is required for feature generation. This is extracted (with the help\n",
    "# of a regular expression)from the raw text obtained from parsing the PDF files.\n",
    "\n",
    "# dictionary of tokenised documents with key as the file name and value as the list of tokens.\n",
    "tokenized_data_title = {}\n",
    "\n",
    "for paper in raw_text:\n",
    "    \n",
    "    # get the title of the paper for feature extraction\n",
    "    # the regular expression identifies the group of text between A-Z or a-z or '(' and Authored by.\n",
    "    paper_title = get_paper_content(raw_text[paper], '(^[A-Za-z(][\\s\\S]*)\\\\n\\\\nAuthored by')\n",
    "    \n",
    "    # tokens are all normalised to lowercase.\n",
    "    normalised_text = paper_title.lower()\n",
    "    \n",
    "    # list of tokens obtained after processing the document are assigned to the respective document\n",
    "    tokenized_data_title[paper] = get_tokens(normalised_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors of each paper is required for feature generation. This is extracted (with the help\n",
    "# of a regular expression) from the raw text obtained from parsing the PDF files.\n",
    "\n",
    "# dictionary of tokenised documents with key as the file name and value as the list of authors \n",
    "# is generated.\n",
    "authors_data = {}\n",
    "\n",
    "for paper in raw_text:\n",
    "    \n",
    "    # from the processed PDF files, extract the authors on the basis of the regular expression.\n",
    "    # regular expression: identifies the group that starts between Authored by and Abstract\n",
    "    paper_author = re.search('Authored by:([\\s\\S]*)\\\\n\\\\nAbstract', raw_text[paper]).group(1)\n",
    "    \n",
    "    # split the retrived names using '\\n' to create a list of authors \n",
    "    authors = paper_author.split('\\n')\n",
    "    \n",
    "    # remove empty strings from the list that were created due to splitting\n",
    "    authors = list(filter(None, authors))\n",
    "    \n",
    "    authors_data[paper] = authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   This function computes the frequency of each word in the given token list.\n",
    "#\n",
    "#   :param tokenized_data:    list of tokens without context independent stop words\n",
    "#          content_type:      type to which the tokens belong: Abstract, Title or Authours\n",
    "#\n",
    "#   :return top_ten:    list of top 10 most frequent terms \n",
    "\n",
    "def get_most_common_words(tokenized_data, content_type):\n",
    "    \n",
    "    # Check content type and filter stopwords for abstract and title\n",
    "    if content_type == 'abstract' or content_type == 'title':\n",
    "        tokenized_data = filter_stopwords(tokenized_data)\n",
    "    \n",
    "    # create a list of words from all the documents\n",
    "    words = list(chain.from_iterable(tokenized_data.values()))\n",
    "    \n",
    "    # retrieve the 10 most common words\n",
    "    freq_dist = FreqDist(words)\n",
    "    most_common = freq_dist.most_common(10)\n",
    "    \n",
    "    # Create a list of top 10 common words\n",
    "    top_ten = []\n",
    "    for word in most_common:\n",
    "        top_ten.append(word[0])\n",
    "    \n",
    "    return top_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 10 most occurring terms in abstract\n",
    "top_abstract = get_most_common_words(tokenized_data_abstract,'abstract')\n",
    "\n",
    "# get top 10 most occurring terms in title\n",
    "top_title = get_most_common_words(tokenized_data_title,'title')\n",
    "\n",
    "# get top 10 authors\n",
    "top_author = get_most_common_words(authors_data,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for the statistics\n",
    "stats_data = {'top10_terms_in_abstracts':top_abstract,'top10_terms_in_titles':top_title,'top10_authors':top_author}\n",
    "\n",
    "data_frame = pd.DataFrame(stats_data)\n",
    "\n",
    "# write to csv\n",
    "data_frame.to_csv(\"Group113_stats.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary\n",
    "\n",
    "This code performs two tasks of text-processing:\n",
    "1. Sparse Feature Generation\n",
    "2. Statistics Generation\n",
    "\n",
    "The below sequence of steps has been implements for intial text pre-processing:\n",
    "1. Sentence Segmentation\n",
    "2. Tokens are normalized to lowercase except the capital tokens appearing in the middle of a sentence/line. \n",
    "3. The word are tokenized using the following regular expression, r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\"\n",
    "4. 200 meaningful bigrams are extracted.\n",
    "5. Context-independent and context-dependent (with the threshold set to %95) stop words are removed from the vocabulary.\n",
    "6. Tokens with length less than 3 are removed from the vocabulary.\n",
    "7. Rare tokens (with the threshold set to 3%) are further removed from the vocabulary.\n",
    "8. Stemming of unigram tokens using Porter Stemmer.\n",
    "\n",
    "There were **2250** features obtained after processing the paper bodies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. References\n",
    "\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "2. https://www.nltk.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
